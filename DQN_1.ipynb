{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN4XLolh9EF2FBSq06w5e69",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goya5858/OSERO/blob/main/DQN_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3q0ri9irrIz"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "import torch.multiprocessing as mp\n",
        "import time\n",
        "\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.nn import functional as F\n",
        "\n",
        "BATCH_SIZE=32"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYQW6h6ntWps"
      },
      "source": [
        "# Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M433CP963fxG"
      },
      "source": [
        "class Networks(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(in_features=4, out_features=100)\n",
        "        self.act1 = nn.ReLU()\n",
        "        #self.drop1 = nn.Dropout(p=0.3)\n",
        "\n",
        "        self.linear2 = nn.Linear(in_features=100, out_features=100)\n",
        "        self.act2 = nn.ReLU()\n",
        "        #self.drop2 = nn.Dropout(p=0.3)\n",
        "\n",
        "        self.Q_network = nn.Linear(in_features=100, out_features=2)\n",
        "        #self.pi_softmax = nn.Softmax()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.act1(x)\n",
        "        #x = self.drop1(x)\n",
        "\n",
        "        x = self.linear2(x)\n",
        "        x = self.act2(x)\n",
        "        #x = self.drop2(x)\n",
        "\n",
        "        Q_value = self.Q_network(x)\n",
        "        return Q_value"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWENJVfEE0UZ"
      },
      "source": [
        "class Agetnt_Memory():\n",
        "    def __init__(self):\n",
        "        self.States   = []\n",
        "        self.Actions = []\n",
        "        self.Rewords = []\n",
        "        self.Next_States = []\n",
        "    \n",
        "    def put_memory(self, state, action, reword, next_state):\n",
        "        self.States.append(state)\n",
        "        self.Actions.append(action)\n",
        "        self.Rewords.append(reword)\n",
        "        self.Next_States.append(next_state)\n",
        "\n",
        "        if len(self.Actions) > 5000:\n",
        "            self.States   = self.States[:-5000]\n",
        "            self.Actions = self.Actions[:-5000]\n",
        "            self.Rewords = self.Rewords[:-5000]\n",
        "            self.Next_States = self.Next_States[:-5000]\n",
        "\n",
        "    def get_batch(self, BATCH_SIZE):\n",
        "        num_memories = len(self.States)\n",
        "        sample_index = random.sample( range(num_memories), min([BATCH_SIZE, num_memories]) )\n",
        "\n",
        "        print(np.stack( np.array(self.States)[sample_index]  ))\n",
        "        States_BATCH = torch.tensor( np.stack( np.array(self.States)[sample_index]  ) ).to(device)\n",
        "        Action_BATCH = torch.tensor( np.stack( np.array(self.Actions)[sample_index] ) ).to(device).reshape(-1,1)\n",
        "        Reword_BATCH = torch.tensor( np.stack( np.array(self.Rewords)[sample_index] ) ).to(device)\n",
        "        #Next_States_BATCH = torch.tensor( np.stack( np.array(self.Next_States)[sample_index] ) ).to(device)\n",
        "        Next_States_BATCH = np.array(self.Next_States)[sample_index]\n",
        "        Next_S_non_mask = [s is not None for s in Next_States_BATCH]\n",
        "        Next_States_BATCH = torch.tensor( np.stack(Next_States_BATCH[Next_S_non_mask]) ).to(device)\n",
        "        return States_BATCH, \\\n",
        "               Action_BATCH, \\\n",
        "               Reword_BATCH, \\\n",
        "               Next_States_BATCH, \\\n",
        "               Next_S_non_mask"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXZey65_tVYq"
      },
      "source": [
        "class Agent:\n",
        "    # 実際に行動を行うAgent\n",
        "    # DQNで学習する\n",
        "    def __init__(self):\n",
        "        self.gamma = torch.tensor(0.99).to(device)\n",
        "        self.episode = 0\n",
        "\n",
        "        self.Memory = Agetnt_Memory()\n",
        "\n",
        "        self.Network = Networks().double().to(device)\n",
        "        self.Network_target = Networks().double().to(device)\n",
        "        self.Network_target.load_state_dict( self.Network.state_dict() )\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(self.Network.parameters(), 0.1)\n",
        "        self.optim = torch.optim.Adam(params=self.Network.parameters(), lr=0.0005 )\n",
        "    \n",
        "    def get_action(self, state):\n",
        "        #ε-greedy法で行動を決定します\n",
        "        #stateを入力として受け取り、actionを出力として返す\n",
        "        #if self.episode >= 500:   \n",
        "        #    eps = 0.0\n",
        "        #else:\n",
        "        #    eps = 0.3 * (1 - self.episode/500)  # linearly interpolate\n",
        "\n",
        "        #if random.random() < eps:\n",
        "        if 0 > 1:\n",
        "            #action = random.randint(0, 2 - 1)   # ランダムに行動\n",
        "            pass\n",
        "        else:\n",
        "            Q_values = self.Network( torch.tensor(state).double().to(device) )\n",
        "            p_values = nn.Softmax()( Q_values )\n",
        "            action = np.random.choice(2, p=p_values.cpu().detach().numpy()[0])\n",
        "        return action\n",
        "\n",
        "    def put_memory(self, state, action, reword, next_state):\n",
        "        self.Memory.put_memory(state, action, reword, next_state)\n",
        "\n",
        "    def network_update(self, BATCH_SIZE=BATCH_SIZE):\n",
        "        # AgentのMemoriesからBATCH_SIZEの分だけデータをランダムに取り出す\n",
        "        States_BATCH, Action_BATCH, Reword_BATCH, Next_States_BATCH, Next_S_non_mask = self.Memory.get_batch(BATCH_SIZE)\n",
        "\n",
        "        Q_values = self.Network( States_BATCH )\n",
        "        Next_Q_values = torch.zeros_like( Q_values )\n",
        "\n",
        "        Q_values = Q_values.gather( -1, Action_BATCH ) #Gatherメソッドでdim=1方向でそれぞれの行でスライスできる\n",
        "\n",
        "        Next_Q_values[Next_S_non_mask] = self.Network_target( Next_States_BATCH )\n",
        "        Next_Q_values = Next_Q_values.max(dim=1).values\n",
        "\n",
        "        True_Values = Reword_BATCH + self.gamma * Next_Q_values\n",
        "        loss = nn.MSELoss()(Q_values, True_Values)\n",
        "    \n",
        "        self.optim.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optim.step()\n",
        "        #print('loss :', loss.cpu().detach())\n",
        "    \n",
        "    def reset_model(self, episode):\n",
        "        self.episode = episode\n",
        "        self.Network_target.load_state_dict( self.Network.state_dict() )"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSddpCbOF-pd"
      },
      "source": [
        "# namedtupleを生成\n",
        "from collections import namedtuple\n",
        "\n",
        "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "# 経験を保存するメモリクラスを定義します\n",
        "class ReplayMemory:\n",
        "    def __init__(self, CAPACITY):\n",
        "        self.capacity = CAPACITY  # メモリの最大長さ\n",
        "        self.memory = []  # 経験を保存する変数\n",
        "        self.index = 0  # 保存するindexを示す変数\n",
        "\n",
        "    def push(self, state, action, state_next, reward):\n",
        "        '''transition = (state, action, state_next, reward)をメモリに保存する'''\n",
        "\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)  # メモリが満タンでないときは足す\n",
        "\n",
        "        self.memory[self.index] = Transition(state, action, state_next, reward)\n",
        "\n",
        "        self.index = (self.index + 1) % self.capacity  # 保存するindexを1つずらす\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        '''batch_size分だけ、ランダムに保存内容を取り出す'''\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        '''関数lenに対して、現在の変数memoryの長さを返す'''\n",
        "        return len(self.memory)\n",
        "\n",
        "# エージェントが持つ脳となるクラスです、DQNを実行します\n",
        "# Q関数をディープラーニングのネットワークをクラスとして定義\n",
        "\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "CAPACITY = 10000\n",
        "\n",
        "\n",
        "class Brain:\n",
        "    def __init__(self, num_states, num_actions):\n",
        "        self.num_actions = num_actions  # CartPoleの行動（右に左に押す）の2を取得\n",
        "\n",
        "        # 経験を記憶するメモリオブジェクトを生成\n",
        "        self.memory = ReplayMemory(CAPACITY)\n",
        "\n",
        "        # ニューラルネットワークを構築\n",
        "        self.model = nn.Sequential()\n",
        "        self.model.add_module('fc1', nn.Linear(num_states, 32))\n",
        "        self.model.add_module('relu1', nn.ReLU())\n",
        "        self.model.add_module('fc2', nn.Linear(32, 32))\n",
        "        self.model.add_module('relu2', nn.ReLU())\n",
        "        self.model.add_module('fc3', nn.Linear(32, num_actions))\n",
        "\n",
        "        print(self.model)  # ネットワークの形を出力\n",
        "\n",
        "        # 最適化手法の設定\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.0001)\n",
        "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.1)\n",
        "\n",
        "    def replay(self):\n",
        "        '''Experience Replayでネットワークの結合パラメータを学習'''\n",
        "\n",
        "        if len(self.memory) < BATCH_SIZE:\n",
        "            return\n",
        "\n",
        "        transitions = self.memory.sample(BATCH_SIZE)\n",
        "        batch = Transition(*zip(*transitions))\n",
        "\n",
        "        state_batch = torch.cat(batch.state)\n",
        "        action_batch = torch.cat(batch.action)\n",
        "        reward_batch = torch.cat(batch.reward)\n",
        "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                           if s is not None])\n",
        "\n",
        "        state_action_values = self.model(state_batch).gather(1, action_batch)\n",
        "\n",
        "        non_final_mask = torch.ByteTensor(tuple(map(lambda s: s is not None, batch.next_state)))\n",
        "        # まずは全部0にしておく\n",
        "        next_state_values = torch.zeros(BATCH_SIZE)\n",
        "        next_state_values[non_final_mask] = self.model(non_final_next_states).max(1)[0].detach()\n",
        "\n",
        "        expected_state_action_values = reward_batch + GAMMA * next_state_values\n",
        "\n",
        "        loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "        # 4.3 結合パラメータを更新する\n",
        "        self.optimizer.zero_grad()  # 勾配をリセット\n",
        "        loss.backward()  # バックプロパゲーションを計算\n",
        "        self.optimizer.step()  # 結合パラメータを更新\n",
        "\n",
        "    def decide_action(self, state, episode):\n",
        "        '''現在の状態に応じて、行動を決定する'''\n",
        "        # ε-greedy法で徐々に最適行動のみを採用する\n",
        "        epsilon = 0.5 * (1 / (episode + 1))\n",
        "\n",
        "        if epsilon <= np.random.uniform(0, 1):\n",
        "            self.model.eval()  # ネットワークを推論モードに切り替える\n",
        "            with torch.no_grad():\n",
        "                action = self.model(state).max(1)[1].view(1, 1)\n",
        "            # ネットワークの出力の最大値のindexを取り出します = max(1)[1]\n",
        "            # .view(1,1)は[torch.LongTensor of size 1]　を size 1x1 に変換します\n",
        "\n",
        "        else:\n",
        "            # 0,1の行動をランダムに返す\n",
        "            action = torch.LongTensor(\n",
        "                [[random.randrange(self.num_actions)]])  # 0,1の行動をランダムに返す\n",
        "            # actionは[torch.LongTensor of size 1x1]の形になります\n",
        "\n",
        "        return action\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, num_states, num_actions):\n",
        "        '''課題の状態と行動の数を設定する'''\n",
        "        self.brain = Brain(num_states, num_actions)  # エージェントが行動を決定するための頭脳を生成\n",
        "\n",
        "    def update_q_function(self):\n",
        "        '''Q関数を更新する'''\n",
        "        self.brain.replay()\n",
        "\n",
        "    def get_action(self, state, episode):\n",
        "        '''行動を決定する'''\n",
        "        action = self.brain.decide_action(state, episode)\n",
        "        return action\n",
        "\n",
        "    def memorize(self, state, action, state_next, reward):\n",
        "        '''memoryオブジェクトに、state, action, state_next, rewardの内容を保存する'''\n",
        "        self.brain.memory.push(state, action, state_next, reward)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1PZVQnhA-T8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr8OBm06A-wO"
      },
      "source": [
        "from collections import namedtuple\n",
        "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'state_next'))\n",
        "\n",
        "class Agent_Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1  = nn.Linear(4, 32)\n",
        "        self.act1 = nn.ReLU()\n",
        "\n",
        "        self.fc2  = nn.Linear(32, 32)\n",
        "        self.act2 = nn.ReLU()\n",
        "\n",
        "        self.fc3  = nn.Linear(32, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act1(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.act2(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "class Agent_Memory:\n",
        "    def __init__(self):\n",
        "        self.capacity = CAPACITY #メモリの大きさ\n",
        "        self.memory = []\n",
        "        self.index = 0\n",
        "\n",
        "    def sample(self, BATCH_SIZE):\n",
        "        '''batch_size分だけ、ランダムに保存内容を取り出す'''\n",
        "        if len(self.memory) < BATCH_SIZE:\n",
        "            BATCH_SIZE = len(self.memory)\n",
        "\n",
        "        transitions = random.sample(self.memory, BATCH_SIZE) #BATCH_SIZEの分だけランダムにデータを取り出す\n",
        "        BATCH = Transition(*zip(*transitions)) #State, Action, Reward, State_Nextの各要素ごとにまとめ直す\n",
        "\n",
        "        State_BATCH      = torch.tensor( np.stack( BATCH.state ) ,dtype=torch.float)\n",
        "        Action_BATCH     = torch.tensor( np.stack( BATCH.action ) ,dtype=torch.int64)\n",
        "        Reward_BATCH     = torch.tensor( np.stack( BATCH.reward ) ,dtype=torch.float)\n",
        "        State_Next_BATCH = np.array( BATCH.state_next ) \n",
        "        non_final_mask   = [s is not None for s in State_Next_BATCH]\n",
        "        State_Next_BATCH = torch.tensor( np.stack(State_Next_BATCH[non_final_mask] ) ,dtype=torch.float)\n",
        "        return State_BATCH, Action_BATCH, Reward_BATCH, State_Next_BATCH, non_final_mask\n",
        "\n",
        "\n",
        "\n",
        "    def put_memory(self, state, action, reward, state_next):\n",
        "        '''transition = (state, action, reward, state_next)をメモリに保存する'''\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)  # メモリが満タンでないときは足す\n",
        "        self.memory[self.index] = Transition(state, action, reward, state_next)\n",
        "        self.index = (self.index + 1) % self.capacity  # 保存するindexを1つずらす\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.Network = Agent_Network()\n",
        "        self.Memory  = Agent_Memory()\n",
        "        self.optim   = optim.Adam(self.Network.parameters(), lr=0.0001)\n",
        "\n",
        "    def get_action(self, state, episode):\n",
        "        state = torch.tensor( state, dtype=torch.float )\n",
        "        # ε-greedy法で徐々に最適行動のみを採用する\n",
        "        epsilon = 0.5 * (1 / (episode + 1))\n",
        "\n",
        "        if epsilon <= np.random.uniform(0, 1):\n",
        "            self.Network.eval()  # ネットワークを推論モードに切り替える\n",
        "            with torch.no_grad():\n",
        "                Q_values = self.Network( state )\n",
        "                action = Q_values.argmax().detach().numpy()\n",
        "                print('OK')\n",
        "        else:\n",
        "            # 0,1の行動をランダムに返す\n",
        "            action = np.random.choice(2)   # 0,1の行動をランダムに返す\n",
        "            action = np.array(action)\n",
        "        return action\n",
        "\n",
        "    def update_Network(self, BATCH_SIZE):\n",
        "        if len(self.Memory.memory) < BATCH_SIZE:\n",
        "            BATCH_SIZE = len(self.Memory.memory)\n",
        "\n",
        "        # Memoryから学習用のデータをランダムに取り出す\n",
        "        State_BATCH, Action_BATCH, Reward_BATCH, State_Next_BATCH, non_final_mask = self.Memory.sample(BATCH_SIZE)\n",
        "        Q_values = self.Network(State_BATCH).gather(1, Action_BATCH.reshape(-1,1)) # Action_BATCHのshapeは(-1)ではなく、(-1,1)の二次元である必要がある\n",
        "        \n",
        "        next_Q_values = torch.zeros(BATCH_SIZE) #全部0で初期化しておいて、non_finalの場所だけ計算する\n",
        "        next_Q_values[non_final_mask] = self.Network(State_Next_BATCH).max(1)[0].detach()\n",
        "        # max()メソッドで最大値を計算した後、[0]で数値を取り出す必要がある, detach()で勾配を消しておく    \n",
        "\n",
        "        true_Q_values =  (Reward_BATCH + gamma * next_Q_values).unsqueeze(1)  #2次元配列に変換\n",
        "        loss = F.smooth_l1_loss(Q_values, true_Q_values)\n",
        "\n",
        "        self.optim.zero_grad()  # 勾配をリセット\n",
        "        loss.backward()  # バックプロパゲーションを計算\n",
        "        self.optim.step()  # 結合パラメータを更新"
      ],
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fh0qXmSFx9a",
        "outputId": "84fa8baa-f324-4f3a-b521-8f0e205ccc25"
      },
      "source": [
        "AA = Agent()\n",
        "BATCH_SIZE = 10\n",
        "gamma = 0.99\n",
        "\n",
        "for i in range(10):\n",
        "    i = np.array(i)\n",
        "    if i != 9:\n",
        "        AA.Memory.put_memory( np.array((i,i,i,i)) ,np.array(0) ,i , np.array((i+1,i+1,i+1,i+1)) )\n",
        "    else:\n",
        "        AA.Memory.put_memory( np.array((i,i,i,i)), np.array(0), i, None )\n",
        "\n",
        "AA.get_action([0,0,0,0], 1)"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZ1enWsMM1V5",
        "outputId": "d8343363-373d-4e31-e5dd-caaa2fc18e90"
      },
      "source": [
        "Action_BATCH"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 9, 6, 2, 4, 7, 1, 8, 5, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkxVf_iQtYpq"
      },
      "source": [
        "# Task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_6T4fAHtaKD"
      },
      "source": [
        "Task_model = gym.make('CartPole-v0')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0fqHXGuthsK"
      },
      "source": [
        "# Training Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92Q2VgVVsd0j"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self):\n",
        "        self.Task  = Task_model\n",
        "        self.Agent = Agent(4, 2)\n",
        "    \n",
        "    def run_one_episode(self, episode):\n",
        "        # 倒れるか、200秒持ち堪え耐えるまでが１エピソード\n",
        "        # 最初にモデルに今回のState_t, 前回のReword_t-1 を渡す。\n",
        "        # AgentはState_tを元にV_t, pi_t を計算して記憶する\n",
        "        \n",
        "        t = 0\n",
        "        state = self.Task.reset() # self.TaskからState_tを受け取る\n",
        "        state = torch.from_numpy(state).type(torch.FloatTensor)  # NumPy変数をPyTorchのテンソルに変換\n",
        "        state = torch.unsqueeze(state, 0)  # size 4をsize 1x4に変換\n",
        "        reword = 0\n",
        "\n",
        "        while True:\n",
        "            action  = self.Agent.get_action(state, 500)\n",
        "            next_state, reword, done, info = self.Task.step(action.item()) #Action_tを場面に渡してState_t+1, Reword_tを受け取る\n",
        "            next_state = torch.from_numpy(next_state).type(torch.FloatTensor)  # NumPy変数をPyTorchのテンソルに変換\n",
        "            next_state = torch.unsqueeze(next_state, 0)  # size 4をsize 1x4に変換\n",
        "            \n",
        "            if done: #倒れていたor終了した場合\n",
        "                next_state = None\n",
        "                if t < 195: #195ステップいないなら失敗\n",
        "                    reword = torch.FloatTensor([-1.0])\n",
        "                else: #195ステップ以上たってたら成功\n",
        "                    reword = torch.FloatTensor([1.0])\n",
        "\n",
        "            else: #各ステップで立ってたら報酬追加\n",
        "                reword = torch.FloatTensor([0.0])\n",
        "            \n",
        "            self.Agent.memorize(state, action, next_state, reword)\n",
        "            self.Agent.update_q_function()\n",
        "\n",
        "            state = next_state\n",
        "            t += 1\n",
        "\n",
        "            if done:\n",
        "                #print(self.Agent.V_values)\n",
        "                #self.Agent.reset_model(episode)\n",
        "                break\n",
        "        return t\n",
        "    \n",
        "    def train(self):\n",
        "        TIMES = []\n",
        "        times = 0\n",
        "        episode = 0\n",
        "        while True:\n",
        "            times = self.run_one_episode(episode)\n",
        "            print('episode :', episode)\n",
        "            print(times)\n",
        "            print('='*10)\n",
        "            episode += 1\n",
        "            TIMES.append(times)\n",
        "\n",
        "            if (len(TIMES) > 10) & (np.mean(TIMES[:-10]) > 150):\n",
        "                \n",
        "                break\n",
        "        return TIMES"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2yNnUKhYExB"
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS1M4H4CKPQl"
      },
      "source": [
        "GAMMA = 0.99\n",
        "AAA = Environment()\n",
        "TTT = AAA.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nmRvXaPfsNR"
      },
      "source": [
        "X = range(len(TTT))\n",
        "plt.plot(X, TTT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beq4hB-O0jXB"
      },
      "source": [
        "AAA.Agent.Next_States"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05OazqYs0ZYi",
        "outputId": "3cce737d-be69-474a-c838-7e85549858c4"
      },
      "source": [
        "len(AAA.Agent.States)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21767"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycyRDgzoOQFJ"
      },
      "source": [
        "# AAA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pi0MwvcAORph",
        "outputId": "b45bf58a-26b7-4875-fd7b-76f4050cf338"
      },
      "source": [
        "!pip install JSAnimation\n",
        "\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from matplotlib import animation\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1]/72.0, frames[0].shape[0]/72.0),\n",
        "               dpi=72)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames),\n",
        "                                   interval=50)\n",
        "\n",
        "    anim.save('movie_cartpole_DQN.mp4')  # 動画のファイル名と保存です\n",
        "    display(display_animation(anim, default_mode='loop'))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting JSAnimation\n",
            "  Downloading https://files.pythonhosted.org/packages/3c/e6/a93a578400c38a43af8b4271334ed2444b42d65580f1d6721c9fe32e9fd8/JSAnimation-0.1.tar.gz\n",
            "Building wheels for collected packages: JSAnimation\n",
            "  Building wheel for JSAnimation (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for JSAnimation: filename=JSAnimation-0.1-cp37-none-any.whl size=11427 sha256=4f0fe318de78e3baabc6b72803f3f4fa69f982ba996d34d10963e0e10d1f40af\n",
            "  Stored in directory: /root/.cache/pip/wheels/3c/c2/b2/b444dffc3eed9c78139288d301c4009a42c0dd061d3b62cead\n",
            "Successfully built JSAnimation\n",
            "Installing collected packages: JSAnimation\n",
            "Successfully installed JSAnimation-0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-xLYS5kOqhz"
      },
      "source": [
        "# 定数の設定\n",
        "ENV = 'CartPole-v0'  # 使用する課題名\n",
        "GAMMA = 0.99  # 時間割引率\n",
        "MAX_STEPS = 200  # 1試行のstep数\n",
        "NUM_EPISODES = 500  # 最大試行回数\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JDbJ6YLOujz"
      },
      "source": [
        ""
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9SllKjcOxbX"
      },
      "source": [
        "# namedtupleを生成\n",
        "from collections import namedtuple\n",
        "\n",
        "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "# 経験を保存するメモリクラスを定義します\n",
        "class ReplayMemory:\n",
        "    def __init__(self, CAPACITY):\n",
        "        self.capacity = CAPACITY  # メモリの最大長さ\n",
        "        self.memory = []  # 経験を保存する変数\n",
        "        self.index = 0  # 保存するindexを示す変数\n",
        "\n",
        "    def push(self, state, action, state_next, reward):\n",
        "        '''transition = (state, action, state_next, reward)をメモリに保存する'''\n",
        "\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)  # メモリが満タンでないときは足す\n",
        "\n",
        "        self.memory[self.index] = Transition(state, action, state_next, reward)\n",
        "\n",
        "        self.index = (self.index + 1) % self.capacity  # 保存するindexを1つずらす\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        '''batch_size分だけ、ランダムに保存内容を取り出す'''\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        '''関数lenに対して、現在の変数memoryの長さを返す'''\n",
        "        return len(self.memory)\n",
        "\n",
        "# エージェントが持つ脳となるクラスです、DQNを実行します\n",
        "# Q関数をディープラーニングのネットワークをクラスとして定義\n",
        "\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "CAPACITY = 10000\n",
        "\n",
        "\n",
        "class Brain:\n",
        "    def __init__(self, num_states, num_actions):\n",
        "        self.num_actions = num_actions  # CartPoleの行動（右に左に押す）の2を取得\n",
        "\n",
        "        # 経験を記憶するメモリオブジェクトを生成\n",
        "        self.memory = ReplayMemory(CAPACITY)\n",
        "\n",
        "        # ニューラルネットワークを構築\n",
        "        self.model = nn.Sequential()\n",
        "        self.model.add_module('fc1', nn.Linear(num_states, 32))\n",
        "        self.model.add_module('relu1', nn.ReLU())\n",
        "        self.model.add_module('fc2', nn.Linear(32, 32))\n",
        "        self.model.add_module('relu2', nn.ReLU())\n",
        "        self.model.add_module('fc3', nn.Linear(32, num_actions))\n",
        "\n",
        "        print(self.model)  # ネットワークの形を出力\n",
        "\n",
        "        # 最適化手法の設定\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.0001)\n",
        "\n",
        "    def replay(self):\n",
        "        '''Experience Replayでネットワークの結合パラメータを学習'''\n",
        "\n",
        "        if len(self.memory) < BATCH_SIZE:\n",
        "            return\n",
        "\n",
        "        transitions = self.memory.sample(BATCH_SIZE)\n",
        "        batch = Transition(*zip(*transitions))\n",
        "\n",
        "        state_batch = torch.cat(batch.state)\n",
        "        action_batch = torch.cat(batch.action)\n",
        "        reward_batch = torch.cat(batch.reward)\n",
        "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                           if s is not None])\n",
        "\n",
        "        state_action_values = self.model(state_batch).gather(1, action_batch)\n",
        "\n",
        "        non_final_mask = torch.ByteTensor(tuple(map(lambda s: s is not None, batch.next_state)))\n",
        "        # まずは全部0にしておく\n",
        "        next_state_values = torch.zeros(BATCH_SIZE)\n",
        "        next_state_values[non_final_mask] = self.model(non_final_next_states).max(1)[0].detach()\n",
        "\n",
        "        expected_state_action_values = reward_batch + GAMMA * next_state_values\n",
        "\n",
        "        loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "        # 4.3 結合パラメータを更新する\n",
        "        self.optimizer.zero_grad()  # 勾配をリセット\n",
        "        loss.backward()  # バックプロパゲーションを計算\n",
        "        self.optimizer.step()  # 結合パラメータを更新\n",
        "\n",
        "    def decide_action(self, state, episode):\n",
        "        '''現在の状態に応じて、行動を決定する'''\n",
        "        # ε-greedy法で徐々に最適行動のみを採用する\n",
        "        epsilon = 0.5 * (1 / (episode + 1))\n",
        "\n",
        "        if epsilon <= np.random.uniform(0, 1):\n",
        "            self.model.eval()  # ネットワークを推論モードに切り替える\n",
        "            with torch.no_grad():\n",
        "                action = self.model(state).max(1)[1].view(1, 1)\n",
        "            # ネットワークの出力の最大値のindexを取り出します = max(1)[1]\n",
        "            # .view(1,1)は[torch.LongTensor of size 1]　を size 1x1 に変換します\n",
        "\n",
        "        else:\n",
        "            # 0,1の行動をランダムに返す\n",
        "            action = torch.LongTensor(\n",
        "                [[random.randrange(self.num_actions)]])  # 0,1の行動をランダムに返す\n",
        "            # actionは[torch.LongTensor of size 1x1]の形になります\n",
        "\n",
        "        return action"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_VHkVYPO3_d"
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, num_states, num_actions):\n",
        "        '''課題の状態と行動の数を設定する'''\n",
        "        self.brain = Brain(num_states, num_actions)  # エージェントが行動を決定するための頭脳を生成\n",
        "\n",
        "    def update_q_function(self):\n",
        "        '''Q関数を更新する'''\n",
        "        self.brain.replay()\n",
        "\n",
        "    def get_action(self, state, episode):\n",
        "        '''行動を決定する'''\n",
        "        action = self.brain.decide_action(state, episode)\n",
        "        return action\n",
        "\n",
        "    def memorize(self, state, action, state_next, reward):\n",
        "        '''memoryオブジェクトに、state, action, state_next, rewardの内容を保存する'''\n",
        "        self.brain.memory.push(state, action, state_next, reward)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCJFHJTfO4wr"
      },
      "source": [
        "class Environment:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.env = gym.make(ENV)  # 実行する課題を設定\n",
        "        num_states = self.env.observation_space.shape[0]  # 課題の状態数4を取得\n",
        "        num_actions = self.env.action_space.n  # CartPoleの行動（右に左に押す）の2を取得\n",
        "        self.agent = Agent(num_states, num_actions)  # 環境内で行動するAgentを生成\n",
        "\n",
        "        \n",
        "    def run(self):\n",
        "        '''実行'''\n",
        "        episode_10_list = np.zeros(10)  # 10試行分の立ち続けたstep数を格納し、平均ステップ数を出力に利用\n",
        "        complete_episodes = 0  # 195step以上連続で立ち続けた試行数\n",
        "        episode_final = False  # 最後の試行フラグ\n",
        "        frames = []  # 最後の試行を動画にするために画像を格納する変数\n",
        "\n",
        "        for episode in range(NUM_EPISODES):  # 最大試行数分繰り返す\n",
        "            observation = self.env.reset()  # 環境の初期化\n",
        "\n",
        "            state = observation  # 観測をそのまま状態sとして使用\n",
        "            state = torch.from_numpy(state).type(\n",
        "                torch.FloatTensor)  # NumPy変数をPyTorchのテンソルに変換\n",
        "            state = torch.unsqueeze(state, 0)  # size 4をsize 1x4に変換\n",
        "\n",
        "            for step in range(MAX_STEPS):  # 1エピソードのループ\n",
        "\n",
        "                if episode_final is True:  # 最終試行ではframesに各時刻の画像を追加していく\n",
        "                    frames.append(self.env.render(mode='rgb_array'))\n",
        "\n",
        "                action = self.agent.get_action(state, episode)  # 行動を求める\n",
        "\n",
        "                # 行動a_tの実行により、s_{t+1}とdoneフラグを求める\n",
        "                # actionから.item()を指定して、中身を取り出す\n",
        "                observation_next, _, done, _ = self.env.step(\n",
        "                    action.item())  # rewardとinfoは使わないので_にする\n",
        "\n",
        "                # 報酬を与える。さらにepisodeの終了評価と、state_nextを設定する\n",
        "                if done:  # ステップ数が200経過するか、一定角度以上傾くとdoneはtrueになる\n",
        "                    state_next = None  # 次の状態はないので、Noneを格納\n",
        "\n",
        "                    # 直近10episodeの立てたstep数リストに追加\n",
        "                    episode_10_list = np.hstack(\n",
        "                        (episode_10_list[1:], step + 1))\n",
        "\n",
        "                    if step < 195:\n",
        "                        reward = torch.FloatTensor(\n",
        "                            [-1.0])  # 途中でこけたら罰則として報酬-1を与える\n",
        "                        complete_episodes = 0  # 連続成功記録をリセット\n",
        "                    else:\n",
        "                        reward = torch.FloatTensor([1.0])  # 立ったまま終了時は報酬1を与える\n",
        "                        complete_episodes = complete_episodes + 1  # 連続記録を更新\n",
        "                else:\n",
        "                    reward = torch.FloatTensor([0.0])  # 普段は報酬0\n",
        "                    state_next = observation_next  # 観測をそのまま状態とする\n",
        "                    state_next = torch.from_numpy(state_next).type(\n",
        "                        torch.FloatTensor)  # numpy変数をPyTorchのテンソルに変換\n",
        "                    state_next = torch.unsqueeze(state_next, 0)  # size 4をsize 1x4に変換\n",
        "\n",
        "                # メモリに経験を追加\n",
        "                self.agent.memorize(state, action, state_next, reward)\n",
        "\n",
        "                # Experience ReplayでQ関数を更新する\n",
        "                self.agent.update_q_function()\n",
        "\n",
        "                # 観測の更新\n",
        "                state = state_next\n",
        "\n",
        "                # 終了時の処理\n",
        "                if done:\n",
        "                    print('%d Episode: Finished after %d steps：10試行の平均step数 = %.1lf' % (\n",
        "                        episode, step + 1, episode_10_list.mean()))\n",
        "                    break\n",
        "\n",
        "            if episode_final is True:\n",
        "                # 動画を保存と描画\n",
        "                display_frames_as_gif(frames)\n",
        "                break\n",
        "\n",
        "            # 10連続で200step経ち続けたら成功\n",
        "            if complete_episodes >= 10:\n",
        "                print('10回連続成功')\n",
        "                episode_final = True  # 次の試行を描画を行う最終試行とする"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYt_vGrf2v4k"
      },
      "source": [
        "# CartPoleを実行する環境のクラスです\n",
        "\n",
        "class Environment:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.env = gym.make(ENV)  # 実行する課題を設定\n",
        "        num_states = self.env.observation_space.shape[0]  # 課題の状態数4を取得\n",
        "        num_actions = self.env.action_space.n  # CartPoleの行動（右に左に押す）の2を取得\n",
        "        self.agent = Agent()  # 環境内で行動するAgentを生成\n",
        "\n",
        "        \n",
        "    def run(self):\n",
        "        '''実行'''\n",
        "        episode_10_list = np.zeros(10)  # 10試行分の立ち続けたstep数を格納し、平均ステップ数を出力に利用\n",
        "        complete_episodes = 0  # 195step以上連続で立ち続けた試行数\n",
        "        episode_final = False  # 最後の試行フラグ\n",
        "        frames = []  # 最後の試行を動画にするために画像を格納する変数\n",
        "\n",
        "        for episode in range(NUM_EPISODES):  # 最大試行数分繰り返す\n",
        "            observation = self.env.reset()  # 環境の初期化\n",
        "\n",
        "            state = observation  # 観測をそのまま状態sとして使用\n",
        "            #state = torch.from_numpy(state).type(torch.FloatTensor)  # NumPy変数をPyTorchのテンソルに変換\n",
        "            #state = torch.unsqueeze(state, 0)  # size 4をsize 1x4に変換\n",
        "\n",
        "            for step in range(MAX_STEPS):  # 1エピソードのループ\n",
        "                action = self.agent.get_action(state)\n",
        "                observation_next, _, done, _ = self.env.step(action)\n",
        "\n",
        "                # 報酬を与える。さらにepisodeの終了評価と、state_nextを設定する\n",
        "                if done:  # ステップ数が200経過するか、一定角度以上傾くとdoneはtrueになる\n",
        "                    state_next = None  # 次の状態はないので、Noneを格納\n",
        "\n",
        "                    # 直近10episodeの立てたstep数リストに追加\n",
        "                    episode_10_list = np.hstack(\n",
        "                        (episode_10_list[1:], step + 1))\n",
        "\n",
        "                    if step < 195:\n",
        "                        reward = torch.FloatTensor( [-1.0] )  # 途中でこけたら罰則として報酬-1を与える\n",
        "                        complete_episodes = 0  # 連続成功記録をリセット\n",
        "                    else:\n",
        "                        reward = torch.FloatTensor( [1.0] )  # 立ったまま終了時は報酬1を与える\n",
        "                        complete_episodes = complete_episodes + 1  # 連続記録を更新\n",
        "                else:\n",
        "                    reward = torch.FloatTensor( [0.0] )  # 普段は報酬0\n",
        "                    state_next = observation_next  # 観測をそのまま状態とする\n",
        "                    #state_next = torch.from_numpy(state_next).type(torch.FloatTensor)  # numpy変数をPyTorchのテンソルに変換\n",
        "                    #state_next = torch.unsqueeze(state_next, 0)  # size 4をsize 1x4に変換\n",
        "\n",
        "                # メモリに経験を追加\n",
        "                self.agent.put_memory(state, action, reward, state_next)\n",
        "\n",
        "                # Experience ReplayでQ関数を更新する\n",
        "                self.agent.network_update()\n",
        "\n",
        "                # 観測の更新\n",
        "                state = state_next\n",
        "\n",
        "                # 終了時の処理\n",
        "                if done:\n",
        "                    print('%d Episode: Finished after %d steps：10試行の平均step数 = %.1lf' % (\n",
        "                        episode, step + 1, episode_10_list.mean()))\n",
        "                    break\n",
        "\n",
        "            if episode_final is True:\n",
        "                break\n",
        "\n",
        "            # 10連続で200step経ち続けたら成功\n",
        "            if complete_episodes >= 10:\n",
        "                print('10回連続成功')\n",
        "                episode_final = True  # 次の試行を描画を行う最終試行とする"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKSNZjtd30gv"
      },
      "source": [
        "class Agent:\n",
        "    # 実際に行動を行うAgent\n",
        "    # DQNで学習する\n",
        "    def __init__(self):\n",
        "        self.gamma = torch.tensor(0.99)\n",
        "        self.episode = 0\n",
        "\n",
        "        self.Memory = Agetnt_Memory()\n",
        "\n",
        "        self.Network = Networks().double()\n",
        "        self.Network_target = Networks().double()\n",
        "        self.Network_target.load_state_dict( self.Network.state_dict() )\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(self.Network.parameters(), 0.1)\n",
        "        self.optim = torch.optim.Adam(params=self.Network.parameters(), lr=0.0005 )\n",
        "    \n",
        "    def get_action(self, state):\n",
        "        #ε-greedy法で行動を決定します\n",
        "        #stateを入力として受け取り、actionを出力として返す\n",
        "        #if self.episode >= 500:   \n",
        "        #    eps = 0.0\n",
        "        #else:\n",
        "        #    eps = 0.3 * (1 - self.episode/500)  # linearly interpolate\n",
        "\n",
        "        #if random.random() < eps:\n",
        "        if 0 > 1:\n",
        "            #action = random.randint(0, 2 - 1)   # ランダムに行動\n",
        "            pass\n",
        "        else:\n",
        "            Q_values = self.Network( torch.tensor( state ).double() )\n",
        "            p_values = nn.Softmax()( Q_values )\n",
        "            action = np.random.choice(2, p=p_values.cpu().detach().numpy())\n",
        "        return action\n",
        "\n",
        "    def put_memory(self, state, action, reword, next_state):\n",
        "        self.Memory.put_memory(state, action, reword, next_state)\n",
        "\n",
        "    def network_update(self, BATCH_SIZE=BATCH_SIZE):\n",
        "        # AgentのMemoriesからBATCH_SIZEの分だけデータをランダムに取り出す\n",
        "        States_BATCH, Action_BATCH, Reword_BATCH, Next_States_BATCH, Next_S_non_mask = self.Memory.get_batch(BATCH_SIZE)\n",
        "\n",
        "        Q_values = self.Network( States_BATCH.double() )\n",
        "        Next_Q_values = torch.zeros_like( Q_values )\n",
        "\n",
        "        Q_values = Q_values.gather( -1, Action_BATCH ) #Gatherメソッドでdim=1方向でそれぞれの行でスライスできる\n",
        "\n",
        "        Next_Q_values[Next_S_non_mask] = self.Network_target( Next_States_BATCH.double() )\n",
        "        Next_Q_values = Next_Q_values.max(dim=1).values\n",
        "\n",
        "        True_Values = Reword_BATCH + self.gamma * Next_Q_values\n",
        "        loss = nn.MSELoss()(Q_values, True_Values)\n",
        "    \n",
        "        self.optim.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optim.step()\n",
        "        #print('loss :', loss.cpu().detach())\n",
        "    \n",
        "    def reset_model(self, episode):\n",
        "        self.episode = episode\n",
        "        self.Network_target.load_state_dict( self.Network.state_dict() )"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf2jz3Bh393_"
      },
      "source": [
        "class Agetnt_Memory():\n",
        "    def __init__(self):\n",
        "        self.States   = []\n",
        "        self.Actions = []\n",
        "        self.Rewords = []\n",
        "        self.Next_States = []\n",
        "    \n",
        "    def put_memory(self, state, action, reword, next_state):\n",
        "        self.States.append( state )\n",
        "        self.Actions.append(action)\n",
        "        self.Rewords.append(reword)\n",
        "        self.Next_States.append(next_state)\n",
        "\n",
        "        if len(self.Actions) > 5000:\n",
        "            self.States   = self.States[:-5000]\n",
        "            self.Actions = self.Actions[:-5000]\n",
        "            self.Rewords = self.Rewords[:-5000]\n",
        "            self.Next_States = self.Next_States[:-5000]\n",
        "\n",
        "    def get_batch(self, BATCH_SIZE):\n",
        "        num_memories = len(self.States)\n",
        "        sample_index = random.sample( range(num_memories), min([BATCH_SIZE, num_memories]) )\n",
        "\n",
        "        States_BATCH = torch.tensor( np.stack( np.array(self.States)[sample_index]  ) )\n",
        "        Action_BATCH = torch.tensor( np.stack( np.array(self.Actions)[sample_index] ) ).reshape(-1,1)\n",
        "        Reword_BATCH = torch.tensor( np.stack( np.array(self.Rewords)[sample_index] ) )\n",
        "        #Next_States_BATCH = torch.tensor( np.stack( np.array(self.Next_States)[sample_index] ) ).to(device)\n",
        "        Next_States_BATCH = np.array(self.Next_States)[sample_index]\n",
        "        Next_S_non_mask = [s is not None for s in Next_States_BATCH]\n",
        "        Next_States_BATCH = torch.tensor( np.stack(Next_States_BATCH[Next_S_non_mask]) )\n",
        "        return States_BATCH, \\\n",
        "               Action_BATCH, \\\n",
        "               Reword_BATCH, \\\n",
        "               Next_States_BATCH, \\\n",
        "               Next_S_non_mask"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVpIZPnc817i"
      },
      "source": [
        "Q_values = torch.tensor([[[0.0395, 0.0723]]])\n",
        "A = torch.tensor([[1]])\n",
        "\n",
        "Q_values[0].gather( -1, A )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3N2IqMPbO8n2",
        "outputId": "6a55a3ec-732c-4efe-bd26-71c69d910d49"
      },
      "source": [
        "# main クラス\n",
        "cartpole_env = Environment()\n",
        "cartpole_env.run()"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([3])) that is different to the input size (torch.Size([3, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([6])) that is different to the input size (torch.Size([6, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([7])) that is different to the input size (torch.Size([7, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9])) that is different to the input size (torch.Size([9, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([12])) that is different to the input size (torch.Size([12, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([13])) that is different to the input size (torch.Size([13, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([14])) that is different to the input size (torch.Size([14, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([15])) that is different to the input size (torch.Size([15, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([17])) that is different to the input size (torch.Size([17, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([18])) that is different to the input size (torch.Size([18, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([19])) that is different to the input size (torch.Size([19, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([21])) that is different to the input size (torch.Size([21, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([22])) that is different to the input size (torch.Size([22, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([23])) that is different to the input size (torch.Size([23, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([24])) that is different to the input size (torch.Size([24, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([25])) that is different to the input size (torch.Size([25, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([26])) that is different to the input size (torch.Size([26, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([27])) that is different to the input size (torch.Size([27, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([29])) that is different to the input size (torch.Size([29, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([30])) that is different to the input size (torch.Size([30, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([31])) that is different to the input size (torch.Size([31, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 Episode: Finished after 15 steps：10試行の平均step数 = 1.5\n",
            "1 Episode: Finished after 14 steps：10試行の平均step数 = 2.9\n",
            "2 Episode: Finished after 15 steps：10試行の平均step数 = 4.4\n",
            "3 Episode: Finished after 27 steps：10試行の平均step数 = 7.1\n",
            "4 Episode: Finished after 29 steps：10試行の平均step数 = 10.0\n",
            "5 Episode: Finished after 13 steps：10試行の平均step数 = 11.3\n",
            "6 Episode: Finished after 12 steps：10試行の平均step数 = 12.5\n",
            "7 Episode: Finished after 30 steps：10試行の平均step数 = 15.5\n",
            "8 Episode: Finished after 8 steps：10試行の平均step数 = 16.3\n",
            "9 Episode: Finished after 13 steps：10試行の平均step数 = 17.6\n",
            "10 Episode: Finished after 22 steps：10試行の平均step数 = 18.3\n",
            "11 Episode: Finished after 49 steps：10試行の平均step数 = 21.8\n",
            "12 Episode: Finished after 38 steps：10試行の平均step数 = 24.1\n",
            "13 Episode: Finished after 12 steps：10試行の平均step数 = 22.6\n",
            "14 Episode: Finished after 11 steps：10試行の平均step数 = 20.8\n",
            "15 Episode: Finished after 20 steps：10試行の平均step数 = 21.5\n",
            "16 Episode: Finished after 21 steps：10試行の平均step数 = 22.4\n",
            "17 Episode: Finished after 27 steps：10試行の平均step数 = 22.1\n",
            "18 Episode: Finished after 14 steps：10試行の平均step数 = 22.7\n",
            "19 Episode: Finished after 21 steps：10試行の平均step数 = 23.5\n",
            "20 Episode: Finished after 22 steps：10試行の平均step数 = 23.5\n",
            "21 Episode: Finished after 37 steps：10試行の平均step数 = 22.3\n",
            "22 Episode: Finished after 9 steps：10試行の平均step数 = 19.4\n",
            "23 Episode: Finished after 22 steps：10試行の平均step数 = 20.4\n",
            "24 Episode: Finished after 23 steps：10試行の平均step数 = 21.6\n",
            "25 Episode: Finished after 18 steps：10試行の平均step数 = 21.4\n",
            "26 Episode: Finished after 22 steps：10試行の平均step数 = 21.5\n",
            "27 Episode: Finished after 17 steps：10試行の平均step数 = 20.5\n",
            "28 Episode: Finished after 31 steps：10試行の平均step数 = 22.2\n",
            "29 Episode: Finished after 28 steps：10試行の平均step数 = 22.9\n",
            "30 Episode: Finished after 31 steps：10試行の平均step数 = 23.8\n",
            "31 Episode: Finished after 13 steps：10試行の平均step数 = 21.4\n",
            "32 Episode: Finished after 18 steps：10試行の平均step数 = 22.3\n",
            "33 Episode: Finished after 24 steps：10試行の平均step数 = 22.5\n",
            "34 Episode: Finished after 21 steps：10試行の平均step数 = 22.3\n",
            "35 Episode: Finished after 12 steps：10試行の平均step数 = 21.7\n",
            "36 Episode: Finished after 12 steps：10試行の平均step数 = 20.7\n",
            "37 Episode: Finished after 12 steps：10試行の平均step数 = 20.2\n",
            "38 Episode: Finished after 11 steps：10試行の平均step数 = 18.2\n",
            "39 Episode: Finished after 28 steps：10試行の平均step数 = 18.2\n",
            "40 Episode: Finished after 23 steps：10試行の平均step数 = 17.4\n",
            "41 Episode: Finished after 11 steps：10試行の平均step数 = 17.2\n",
            "42 Episode: Finished after 14 steps：10試行の平均step数 = 16.8\n",
            "43 Episode: Finished after 12 steps：10試行の平均step数 = 15.6\n",
            "44 Episode: Finished after 11 steps：10試行の平均step数 = 14.6\n",
            "45 Episode: Finished after 19 steps：10試行の平均step数 = 15.3\n",
            "46 Episode: Finished after 10 steps：10試行の平均step数 = 15.1\n",
            "47 Episode: Finished after 15 steps：10試行の平均step数 = 15.4\n",
            "48 Episode: Finished after 13 steps：10試行の平均step数 = 15.6\n",
            "49 Episode: Finished after 22 steps：10試行の平均step数 = 15.0\n",
            "50 Episode: Finished after 10 steps：10試行の平均step数 = 13.7\n",
            "51 Episode: Finished after 23 steps：10試行の平均step数 = 14.9\n",
            "52 Episode: Finished after 21 steps：10試行の平均step数 = 15.6\n",
            "53 Episode: Finished after 16 steps：10試行の平均step数 = 16.0\n",
            "54 Episode: Finished after 22 steps：10試行の平均step数 = 17.1\n",
            "55 Episode: Finished after 8 steps：10試行の平均step数 = 16.0\n",
            "56 Episode: Finished after 23 steps：10試行の平均step数 = 17.3\n",
            "57 Episode: Finished after 36 steps：10試行の平均step数 = 19.4\n",
            "58 Episode: Finished after 33 steps：10試行の平均step数 = 21.4\n",
            "59 Episode: Finished after 11 steps：10試行の平均step数 = 20.3\n",
            "60 Episode: Finished after 14 steps：10試行の平均step数 = 20.7\n",
            "61 Episode: Finished after 10 steps：10試行の平均step数 = 19.4\n",
            "62 Episode: Finished after 26 steps：10試行の平均step数 = 19.9\n",
            "63 Episode: Finished after 31 steps：10試行の平均step数 = 21.4\n",
            "64 Episode: Finished after 21 steps：10試行の平均step数 = 21.3\n",
            "65 Episode: Finished after 35 steps：10試行の平均step数 = 24.0\n",
            "66 Episode: Finished after 23 steps：10試行の平均step数 = 24.0\n",
            "67 Episode: Finished after 12 steps：10試行の平均step数 = 21.6\n",
            "68 Episode: Finished after 11 steps：10試行の平均step数 = 19.4\n",
            "69 Episode: Finished after 18 steps：10試行の平均step数 = 20.1\n",
            "70 Episode: Finished after 51 steps：10試行の平均step数 = 23.8\n",
            "71 Episode: Finished after 25 steps：10試行の平均step数 = 25.3\n",
            "72 Episode: Finished after 16 steps：10試行の平均step数 = 24.3\n",
            "73 Episode: Finished after 14 steps：10試行の平均step数 = 22.6\n",
            "74 Episode: Finished after 36 steps：10試行の平均step数 = 24.1\n",
            "75 Episode: Finished after 15 steps：10試行の平均step数 = 22.1\n",
            "76 Episode: Finished after 15 steps：10試行の平均step数 = 21.3\n",
            "77 Episode: Finished after 11 steps：10試行の平均step数 = 21.2\n",
            "78 Episode: Finished after 18 steps：10試行の平均step数 = 21.9\n",
            "79 Episode: Finished after 10 steps：10試行の平均step数 = 21.1\n",
            "80 Episode: Finished after 13 steps：10試行の平均step数 = 17.3\n",
            "81 Episode: Finished after 10 steps：10試行の平均step数 = 15.8\n",
            "82 Episode: Finished after 45 steps：10試行の平均step数 = 18.7\n",
            "83 Episode: Finished after 14 steps：10試行の平均step数 = 18.7\n",
            "84 Episode: Finished after 37 steps：10試行の平均step数 = 18.8\n",
            "85 Episode: Finished after 37 steps：10試行の平均step数 = 21.0\n",
            "86 Episode: Finished after 21 steps：10試行の平均step数 = 21.6\n",
            "87 Episode: Finished after 39 steps：10試行の平均step数 = 24.4\n",
            "88 Episode: Finished after 18 steps：10試行の平均step数 = 24.4\n",
            "89 Episode: Finished after 25 steps：10試行の平均step数 = 25.9\n",
            "90 Episode: Finished after 11 steps：10試行の平均step数 = 25.7\n",
            "91 Episode: Finished after 20 steps：10試行の平均step数 = 26.7\n",
            "92 Episode: Finished after 19 steps：10試行の平均step数 = 24.1\n",
            "93 Episode: Finished after 13 steps：10試行の平均step数 = 24.0\n",
            "94 Episode: Finished after 33 steps：10試行の平均step数 = 23.6\n",
            "95 Episode: Finished after 28 steps：10試行の平均step数 = 22.7\n",
            "96 Episode: Finished after 14 steps：10試行の平均step数 = 22.0\n",
            "97 Episode: Finished after 27 steps：10試行の平均step数 = 20.8\n",
            "98 Episode: Finished after 38 steps：10試行の平均step数 = 22.8\n",
            "99 Episode: Finished after 10 steps：10試行の平均step数 = 21.3\n",
            "100 Episode: Finished after 29 steps：10試行の平均step数 = 23.1\n",
            "101 Episode: Finished after 9 steps：10試行の平均step数 = 22.0\n",
            "102 Episode: Finished after 16 steps：10試行の平均step数 = 21.7\n",
            "103 Episode: Finished after 16 steps：10試行の平均step数 = 22.0\n",
            "104 Episode: Finished after 12 steps：10試行の平均step数 = 19.9\n",
            "105 Episode: Finished after 20 steps：10試行の平均step数 = 19.1\n",
            "106 Episode: Finished after 19 steps：10試行の平均step数 = 19.6\n",
            "107 Episode: Finished after 18 steps：10試行の平均step数 = 18.7\n",
            "108 Episode: Finished after 35 steps：10試行の平均step数 = 18.4\n",
            "109 Episode: Finished after 26 steps：10試行の平均step数 = 20.0\n",
            "110 Episode: Finished after 12 steps：10試行の平均step数 = 18.3\n",
            "111 Episode: Finished after 34 steps：10試行の平均step数 = 20.8\n",
            "112 Episode: Finished after 26 steps：10試行の平均step数 = 21.8\n",
            "113 Episode: Finished after 32 steps：10試行の平均step数 = 23.4\n",
            "114 Episode: Finished after 23 steps：10試行の平均step数 = 24.5\n",
            "115 Episode: Finished after 35 steps：10試行の平均step数 = 26.0\n",
            "116 Episode: Finished after 68 steps：10試行の平均step数 = 30.9\n",
            "117 Episode: Finished after 14 steps：10試行の平均step数 = 30.5\n",
            "118 Episode: Finished after 10 steps：10試行の平均step数 = 28.0\n",
            "119 Episode: Finished after 32 steps：10試行の平均step数 = 28.6\n",
            "120 Episode: Finished after 15 steps：10試行の平均step数 = 28.9\n",
            "121 Episode: Finished after 26 steps：10試行の平均step数 = 28.1\n",
            "122 Episode: Finished after 34 steps：10試行の平均step数 = 28.9\n",
            "123 Episode: Finished after 10 steps：10試行の平均step数 = 26.7\n",
            "124 Episode: Finished after 11 steps：10試行の平均step数 = 25.5\n",
            "125 Episode: Finished after 25 steps：10試行の平均step数 = 24.5\n",
            "126 Episode: Finished after 19 steps：10試行の平均step数 = 19.6\n",
            "127 Episode: Finished after 16 steps：10試行の平均step数 = 19.8\n",
            "128 Episode: Finished after 28 steps：10試行の平均step数 = 21.6\n",
            "129 Episode: Finished after 20 steps：10試行の平均step数 = 20.4\n",
            "130 Episode: Finished after 17 steps：10試行の平均step数 = 20.6\n",
            "131 Episode: Finished after 41 steps：10試行の平均step数 = 22.1\n",
            "132 Episode: Finished after 28 steps：10試行の平均step数 = 21.5\n",
            "133 Episode: Finished after 40 steps：10試行の平均step数 = 24.5\n",
            "134 Episode: Finished after 20 steps：10試行の平均step数 = 25.4\n",
            "135 Episode: Finished after 10 steps：10試行の平均step数 = 23.9\n",
            "136 Episode: Finished after 11 steps：10試行の平均step数 = 23.1\n",
            "137 Episode: Finished after 20 steps：10試行の平均step数 = 23.5\n",
            "138 Episode: Finished after 22 steps：10試行の平均step数 = 22.9\n",
            "139 Episode: Finished after 37 steps：10試行の平均step数 = 24.6\n",
            "140 Episode: Finished after 9 steps：10試行の平均step数 = 23.8\n",
            "141 Episode: Finished after 46 steps：10試行の平均step数 = 24.3\n",
            "142 Episode: Finished after 30 steps：10試行の平均step数 = 24.5\n",
            "143 Episode: Finished after 12 steps：10試行の平均step数 = 21.7\n",
            "144 Episode: Finished after 12 steps：10試行の平均step数 = 20.9\n",
            "145 Episode: Finished after 11 steps：10試行の平均step数 = 21.0\n",
            "146 Episode: Finished after 12 steps：10試行の平均step数 = 21.1\n",
            "147 Episode: Finished after 11 steps：10試行の平均step数 = 20.2\n",
            "148 Episode: Finished after 28 steps：10試行の平均step数 = 20.8\n",
            "149 Episode: Finished after 33 steps：10試行の平均step数 = 20.4\n",
            "150 Episode: Finished after 11 steps：10試行の平均step数 = 20.6\n",
            "151 Episode: Finished after 17 steps：10試行の平均step数 = 17.7\n",
            "152 Episode: Finished after 19 steps：10試行の平均step数 = 16.6\n",
            "153 Episode: Finished after 31 steps：10試行の平均step数 = 18.5\n",
            "154 Episode: Finished after 24 steps：10試行の平均step数 = 19.7\n",
            "155 Episode: Finished after 28 steps：10試行の平均step数 = 21.4\n",
            "156 Episode: Finished after 12 steps：10試行の平均step数 = 21.4\n",
            "157 Episode: Finished after 14 steps：10試行の平均step数 = 21.7\n",
            "158 Episode: Finished after 15 steps：10試行の平均step数 = 20.4\n",
            "159 Episode: Finished after 17 steps：10試行の平均step数 = 18.8\n",
            "160 Episode: Finished after 29 steps：10試行の平均step数 = 20.6\n",
            "161 Episode: Finished after 9 steps：10試行の平均step数 = 19.8\n",
            "162 Episode: Finished after 13 steps：10試行の平均step数 = 19.2\n",
            "163 Episode: Finished after 39 steps：10試行の平均step数 = 20.0\n",
            "164 Episode: Finished after 30 steps：10試行の平均step数 = 20.6\n",
            "165 Episode: Finished after 10 steps：10試行の平均step数 = 18.8\n",
            "166 Episode: Finished after 17 steps：10試行の平均step数 = 19.3\n",
            "167 Episode: Finished after 32 steps：10試行の平均step数 = 21.1\n",
            "168 Episode: Finished after 20 steps：10試行の平均step数 = 21.6\n",
            "169 Episode: Finished after 17 steps：10試行の平均step数 = 21.6\n",
            "170 Episode: Finished after 53 steps：10試行の平均step数 = 24.0\n",
            "171 Episode: Finished after 18 steps：10試行の平均step数 = 24.9\n",
            "172 Episode: Finished after 51 steps：10試行の平均step数 = 28.7\n",
            "173 Episode: Finished after 13 steps：10試行の平均step数 = 26.1\n",
            "174 Episode: Finished after 43 steps：10試行の平均step数 = 27.4\n",
            "175 Episode: Finished after 13 steps：10試行の平均step数 = 27.7\n",
            "176 Episode: Finished after 20 steps：10試行の平均step数 = 28.0\n",
            "177 Episode: Finished after 19 steps：10試行の平均step数 = 26.7\n",
            "178 Episode: Finished after 12 steps：10試行の平均step数 = 25.9\n",
            "179 Episode: Finished after 43 steps：10試行の平均step数 = 28.5\n",
            "180 Episode: Finished after 19 steps：10試行の平均step数 = 25.1\n",
            "181 Episode: Finished after 39 steps：10試行の平均step数 = 27.2\n",
            "182 Episode: Finished after 16 steps：10試行の平均step数 = 23.7\n",
            "183 Episode: Finished after 20 steps：10試行の平均step数 = 24.4\n",
            "184 Episode: Finished after 23 steps：10試行の平均step数 = 22.4\n",
            "185 Episode: Finished after 22 steps：10試行の平均step数 = 23.3\n",
            "186 Episode: Finished after 25 steps：10試行の平均step数 = 23.8\n",
            "187 Episode: Finished after 25 steps：10試行の平均step数 = 24.4\n",
            "188 Episode: Finished after 41 steps：10試行の平均step数 = 27.3\n",
            "189 Episode: Finished after 27 steps：10試行の平均step数 = 25.7\n",
            "190 Episode: Finished after 15 steps：10試行の平均step数 = 25.3\n",
            "191 Episode: Finished after 17 steps：10試行の平均step数 = 23.1\n",
            "192 Episode: Finished after 20 steps：10試行の平均step数 = 23.5\n",
            "193 Episode: Finished after 30 steps：10試行の平均step数 = 24.5\n",
            "194 Episode: Finished after 26 steps：10試行の平均step数 = 24.8\n",
            "195 Episode: Finished after 15 steps：10試行の平均step数 = 24.1\n",
            "196 Episode: Finished after 34 steps：10試行の平均step数 = 25.0\n",
            "197 Episode: Finished after 36 steps：10試行の平均step数 = 26.1\n",
            "198 Episode: Finished after 24 steps：10試行の平均step数 = 24.4\n",
            "199 Episode: Finished after 17 steps：10試行の平均step数 = 23.4\n",
            "200 Episode: Finished after 19 steps：10試行の平均step数 = 23.8\n",
            "201 Episode: Finished after 23 steps：10試行の平均step数 = 24.4\n",
            "202 Episode: Finished after 12 steps：10試行の平均step数 = 23.6\n",
            "203 Episode: Finished after 10 steps：10試行の平均step数 = 21.6\n",
            "204 Episode: Finished after 15 steps：10試行の平均step数 = 20.5\n",
            "205 Episode: Finished after 38 steps：10試行の平均step数 = 22.8\n",
            "206 Episode: Finished after 40 steps：10試行の平均step数 = 23.4\n",
            "207 Episode: Finished after 10 steps：10試行の平均step数 = 20.8\n",
            "208 Episode: Finished after 23 steps：10試行の平均step数 = 20.7\n",
            "209 Episode: Finished after 15 steps：10試行の平均step数 = 20.5\n",
            "210 Episode: Finished after 50 steps：10試行の平均step数 = 23.6\n",
            "211 Episode: Finished after 14 steps：10試行の平均step数 = 22.7\n",
            "212 Episode: Finished after 19 steps：10試行の平均step数 = 23.4\n",
            "213 Episode: Finished after 20 steps：10試行の平均step数 = 24.4\n",
            "214 Episode: Finished after 76 steps：10試行の平均step数 = 30.5\n",
            "215 Episode: Finished after 20 steps：10試行の平均step数 = 28.7\n",
            "216 Episode: Finished after 14 steps：10試行の平均step数 = 26.1\n",
            "217 Episode: Finished after 10 steps：10試行の平均step数 = 26.1\n",
            "218 Episode: Finished after 34 steps：10試行の平均step数 = 27.2\n",
            "219 Episode: Finished after 14 steps：10試行の平均step数 = 27.1\n",
            "220 Episode: Finished after 18 steps：10試行の平均step数 = 23.9\n",
            "221 Episode: Finished after 15 steps：10試行の平均step数 = 24.0\n",
            "222 Episode: Finished after 47 steps：10試行の平均step数 = 26.8\n",
            "223 Episode: Finished after 30 steps：10試行の平均step数 = 27.8\n",
            "224 Episode: Finished after 17 steps：10試行の平均step数 = 21.9\n",
            "225 Episode: Finished after 20 steps：10試行の平均step数 = 21.9\n",
            "226 Episode: Finished after 12 steps：10試行の平均step数 = 21.7\n",
            "227 Episode: Finished after 23 steps：10試行の平均step数 = 23.0\n",
            "228 Episode: Finished after 14 steps：10試行の平均step数 = 21.0\n",
            "229 Episode: Finished after 28 steps：10試行の平均step数 = 22.4\n",
            "230 Episode: Finished after 29 steps：10試行の平均step数 = 23.5\n",
            "231 Episode: Finished after 29 steps：10試行の平均step数 = 24.9\n",
            "232 Episode: Finished after 15 steps：10試行の平均step数 = 21.7\n",
            "233 Episode: Finished after 17 steps：10試行の平均step数 = 20.4\n",
            "234 Episode: Finished after 35 steps：10試行の平均step数 = 22.2\n",
            "235 Episode: Finished after 18 steps：10試行の平均step数 = 22.0\n",
            "236 Episode: Finished after 20 steps：10試行の平均step数 = 22.8\n",
            "237 Episode: Finished after 15 steps：10試行の平均step数 = 22.0\n",
            "238 Episode: Finished after 22 steps：10試行の平均step数 = 22.8\n",
            "239 Episode: Finished after 15 steps：10試行の平均step数 = 21.5\n",
            "240 Episode: Finished after 10 steps：10試行の平均step数 = 19.6\n",
            "241 Episode: Finished after 12 steps：10試行の平均step数 = 17.9\n",
            "242 Episode: Finished after 39 steps：10試行の平均step数 = 20.3\n",
            "243 Episode: Finished after 15 steps：10試行の平均step数 = 20.1\n",
            "244 Episode: Finished after 12 steps：10試行の平均step数 = 17.8\n",
            "245 Episode: Finished after 48 steps：10試行の平均step数 = 20.8\n",
            "246 Episode: Finished after 13 steps：10試行の平均step数 = 20.1\n",
            "247 Episode: Finished after 29 steps：10試行の平均step数 = 21.5\n",
            "248 Episode: Finished after 17 steps：10試行の平均step数 = 21.0\n",
            "249 Episode: Finished after 14 steps：10試行の平均step数 = 20.9\n",
            "250 Episode: Finished after 15 steps：10試行の平均step数 = 21.4\n",
            "251 Episode: Finished after 17 steps：10試行の平均step数 = 21.9\n",
            "252 Episode: Finished after 40 steps：10試行の平均step数 = 22.0\n",
            "253 Episode: Finished after 16 steps：10試行の平均step数 = 22.1\n",
            "254 Episode: Finished after 14 steps：10試行の平均step数 = 22.3\n",
            "255 Episode: Finished after 92 steps：10試行の平均step数 = 26.7\n",
            "256 Episode: Finished after 30 steps：10試行の平均step数 = 28.4\n",
            "257 Episode: Finished after 32 steps：10試行の平均step数 = 28.7\n",
            "258 Episode: Finished after 13 steps：10試行の平均step数 = 28.3\n",
            "259 Episode: Finished after 37 steps：10試行の平均step数 = 30.6\n",
            "260 Episode: Finished after 14 steps：10試行の平均step数 = 30.5\n",
            "261 Episode: Finished after 11 steps：10試行の平均step数 = 29.9\n",
            "262 Episode: Finished after 18 steps：10試行の平均step数 = 27.7\n",
            "263 Episode: Finished after 14 steps：10試行の平均step数 = 27.5\n",
            "264 Episode: Finished after 20 steps：10試行の平均step数 = 28.1\n",
            "265 Episode: Finished after 28 steps：10試行の平均step数 = 21.7\n",
            "266 Episode: Finished after 35 steps：10試行の平均step数 = 22.2\n",
            "267 Episode: Finished after 24 steps：10試行の平均step数 = 21.4\n",
            "268 Episode: Finished after 11 steps：10試行の平均step数 = 21.2\n",
            "269 Episode: Finished after 10 steps：10試行の平均step数 = 18.5\n",
            "270 Episode: Finished after 28 steps：10試行の平均step数 = 19.9\n",
            "271 Episode: Finished after 13 steps：10試行の平均step数 = 20.1\n",
            "272 Episode: Finished after 17 steps：10試行の平均step数 = 20.0\n",
            "273 Episode: Finished after 14 steps：10試行の平均step数 = 20.0\n",
            "274 Episode: Finished after 19 steps：10試行の平均step数 = 19.9\n",
            "275 Episode: Finished after 13 steps：10試行の平均step数 = 18.4\n",
            "276 Episode: Finished after 11 steps：10試行の平均step数 = 16.0\n",
            "277 Episode: Finished after 12 steps：10試行の平均step数 = 14.8\n",
            "278 Episode: Finished after 14 steps：10試行の平均step数 = 15.1\n",
            "279 Episode: Finished after 17 steps：10試行の平均step数 = 15.8\n",
            "280 Episode: Finished after 16 steps：10試行の平均step数 = 14.6\n",
            "281 Episode: Finished after 19 steps：10試行の平均step数 = 15.2\n",
            "282 Episode: Finished after 18 steps：10試行の平均step数 = 15.3\n",
            "283 Episode: Finished after 10 steps：10試行の平均step数 = 14.9\n",
            "284 Episode: Finished after 17 steps：10試行の平均step数 = 14.7\n",
            "285 Episode: Finished after 23 steps：10試行の平均step数 = 15.7\n",
            "286 Episode: Finished after 14 steps：10試行の平均step数 = 16.0\n",
            "287 Episode: Finished after 21 steps：10試行の平均step数 = 16.9\n",
            "288 Episode: Finished after 19 steps：10試行の平均step数 = 17.4\n",
            "289 Episode: Finished after 15 steps：10試行の平均step数 = 17.2\n",
            "290 Episode: Finished after 21 steps：10試行の平均step数 = 17.7\n",
            "291 Episode: Finished after 23 steps：10試行の平均step数 = 18.1\n",
            "292 Episode: Finished after 51 steps：10試行の平均step数 = 21.4\n",
            "293 Episode: Finished after 10 steps：10試行の平均step数 = 21.4\n",
            "294 Episode: Finished after 9 steps：10試行の平均step数 = 20.6\n",
            "295 Episode: Finished after 12 steps：10試行の平均step数 = 19.5\n",
            "296 Episode: Finished after 20 steps：10試行の平均step数 = 20.1\n",
            "297 Episode: Finished after 13 steps：10試行の平均step数 = 19.3\n",
            "298 Episode: Finished after 14 steps：10試行の平均step数 = 18.8\n",
            "299 Episode: Finished after 56 steps：10試行の平均step数 = 22.9\n",
            "300 Episode: Finished after 19 steps：10試行の平均step数 = 22.7\n",
            "301 Episode: Finished after 36 steps：10試行の平均step数 = 24.0\n",
            "302 Episode: Finished after 24 steps：10試行の平均step数 = 21.3\n",
            "303 Episode: Finished after 14 steps：10試行の平均step数 = 21.7\n",
            "304 Episode: Finished after 25 steps：10試行の平均step数 = 23.3\n",
            "305 Episode: Finished after 23 steps：10試行の平均step数 = 24.4\n",
            "306 Episode: Finished after 19 steps：10試行の平均step数 = 24.3\n",
            "307 Episode: Finished after 56 steps：10試行の平均step数 = 28.6\n",
            "308 Episode: Finished after 49 steps：10試行の平均step数 = 32.1\n",
            "309 Episode: Finished after 14 steps：10試行の平均step数 = 27.9\n",
            "310 Episode: Finished after 23 steps：10試行の平均step数 = 28.3\n",
            "311 Episode: Finished after 22 steps：10試行の平均step数 = 26.9\n",
            "312 Episode: Finished after 9 steps：10試行の平均step数 = 25.4\n",
            "313 Episode: Finished after 41 steps：10試行の平均step数 = 28.1\n",
            "314 Episode: Finished after 25 steps：10試行の平均step数 = 28.1\n",
            "315 Episode: Finished after 15 steps：10試行の平均step数 = 27.3\n",
            "316 Episode: Finished after 38 steps：10試行の平均step数 = 29.2\n",
            "317 Episode: Finished after 30 steps：10試行の平均step数 = 26.6\n",
            "318 Episode: Finished after 12 steps：10試行の平均step数 = 22.9\n",
            "319 Episode: Finished after 13 steps：10試行の平均step数 = 22.8\n",
            "320 Episode: Finished after 35 steps：10試行の平均step数 = 24.0\n",
            "321 Episode: Finished after 18 steps：10試行の平均step数 = 23.6\n",
            "322 Episode: Finished after 20 steps：10試行の平均step数 = 24.7\n",
            "323 Episode: Finished after 25 steps：10試行の平均step数 = 23.1\n",
            "324 Episode: Finished after 34 steps：10試行の平均step数 = 24.0\n",
            "325 Episode: Finished after 18 steps：10試行の平均step数 = 24.3\n",
            "326 Episode: Finished after 10 steps：10試行の平均step数 = 21.5\n",
            "327 Episode: Finished after 18 steps：10試行の平均step数 = 20.3\n",
            "328 Episode: Finished after 42 steps：10試行の平均step数 = 23.3\n",
            "329 Episode: Finished after 17 steps：10試行の平均step数 = 23.7\n",
            "330 Episode: Finished after 23 steps：10試行の平均step数 = 22.5\n",
            "331 Episode: Finished after 14 steps：10試行の平均step数 = 22.1\n",
            "332 Episode: Finished after 14 steps：10試行の平均step数 = 21.5\n",
            "333 Episode: Finished after 16 steps：10試行の平均step数 = 20.6\n",
            "334 Episode: Finished after 20 steps：10試行の平均step数 = 19.2\n",
            "335 Episode: Finished after 13 steps：10試行の平均step数 = 18.7\n",
            "336 Episode: Finished after 36 steps：10試行の平均step数 = 21.3\n",
            "337 Episode: Finished after 43 steps：10試行の平均step数 = 23.8\n",
            "338 Episode: Finished after 49 steps：10試行の平均step数 = 24.5\n",
            "339 Episode: Finished after 14 steps：10試行の平均step数 = 24.2\n",
            "340 Episode: Finished after 25 steps：10試行の平均step数 = 24.4\n",
            "341 Episode: Finished after 48 steps：10試行の平均step数 = 27.8\n",
            "342 Episode: Finished after 44 steps：10試行の平均step数 = 30.8\n",
            "343 Episode: Finished after 32 steps：10試行の平均step数 = 32.4\n",
            "344 Episode: Finished after 30 steps：10試行の平均step数 = 33.4\n",
            "345 Episode: Finished after 25 steps：10試行の平均step数 = 34.6\n",
            "346 Episode: Finished after 24 steps：10試行の平均step数 = 33.4\n",
            "347 Episode: Finished after 27 steps：10試行の平均step数 = 31.8\n",
            "348 Episode: Finished after 31 steps：10試行の平均step数 = 30.0\n",
            "349 Episode: Finished after 14 steps：10試行の平均step数 = 30.0\n",
            "350 Episode: Finished after 22 steps：10試行の平均step数 = 29.7\n",
            "351 Episode: Finished after 27 steps：10試行の平均step数 = 27.6\n",
            "352 Episode: Finished after 11 steps：10試行の平均step数 = 24.3\n",
            "353 Episode: Finished after 16 steps：10試行の平均step数 = 22.7\n",
            "354 Episode: Finished after 16 steps：10試行の平均step数 = 21.3\n",
            "355 Episode: Finished after 50 steps：10試行の平均step数 = 23.8\n",
            "356 Episode: Finished after 25 steps：10試行の平均step数 = 23.9\n",
            "357 Episode: Finished after 17 steps：10試行の平均step数 = 22.9\n",
            "358 Episode: Finished after 11 steps：10試行の平均step数 = 20.9\n",
            "359 Episode: Finished after 17 steps：10試行の平均step数 = 21.2\n",
            "360 Episode: Finished after 23 steps：10試行の平均step数 = 21.3\n",
            "361 Episode: Finished after 12 steps：10試行の平均step数 = 19.8\n",
            "362 Episode: Finished after 21 steps：10試行の平均step数 = 20.8\n",
            "363 Episode: Finished after 22 steps：10試行の平均step数 = 21.4\n",
            "364 Episode: Finished after 10 steps：10試行の平均step数 = 20.8\n",
            "365 Episode: Finished after 18 steps：10試行の平均step数 = 17.6\n",
            "366 Episode: Finished after 16 steps：10試行の平均step数 = 16.7\n",
            "367 Episode: Finished after 15 steps：10試行の平均step数 = 16.5\n",
            "368 Episode: Finished after 13 steps：10試行の平均step数 = 16.7\n",
            "369 Episode: Finished after 22 steps：10試行の平均step数 = 17.2\n",
            "370 Episode: Finished after 12 steps：10試行の平均step数 = 16.1\n",
            "371 Episode: Finished after 37 steps：10試行の平均step数 = 18.6\n",
            "372 Episode: Finished after 21 steps：10試行の平均step数 = 18.6\n",
            "373 Episode: Finished after 14 steps：10試行の平均step数 = 17.8\n",
            "374 Episode: Finished after 16 steps：10試行の平均step数 = 18.4\n",
            "375 Episode: Finished after 14 steps：10試行の平均step数 = 18.0\n",
            "376 Episode: Finished after 34 steps：10試行の平均step数 = 19.8\n",
            "377 Episode: Finished after 40 steps：10試行の平均step数 = 22.3\n",
            "378 Episode: Finished after 27 steps：10試行の平均step数 = 23.7\n",
            "379 Episode: Finished after 9 steps：10試行の平均step数 = 22.4\n",
            "380 Episode: Finished after 46 steps：10試行の平均step数 = 25.8\n",
            "381 Episode: Finished after 14 steps：10試行の平均step数 = 23.5\n",
            "382 Episode: Finished after 30 steps：10試行の平均step数 = 24.4\n",
            "383 Episode: Finished after 15 steps：10試行の平均step数 = 24.5\n",
            "384 Episode: Finished after 21 steps：10試行の平均step数 = 25.0\n",
            "385 Episode: Finished after 19 steps：10試行の平均step数 = 25.5\n",
            "386 Episode: Finished after 39 steps：10試行の平均step数 = 26.0\n",
            "387 Episode: Finished after 18 steps：10試行の平均step数 = 23.8\n",
            "388 Episode: Finished after 11 steps：10試行の平均step数 = 22.2\n",
            "389 Episode: Finished after 13 steps：10試行の平均step数 = 22.6\n",
            "390 Episode: Finished after 14 steps：10試行の平均step数 = 19.4\n",
            "391 Episode: Finished after 23 steps：10試行の平均step数 = 20.3\n",
            "392 Episode: Finished after 46 steps：10試行の平均step数 = 21.9\n",
            "393 Episode: Finished after 17 steps：10試行の平均step数 = 22.1\n",
            "394 Episode: Finished after 11 steps：10試行の平均step数 = 21.1\n",
            "395 Episode: Finished after 12 steps：10試行の平均step数 = 20.4\n",
            "396 Episode: Finished after 43 steps：10試行の平均step数 = 20.8\n",
            "397 Episode: Finished after 22 steps：10試行の平均step数 = 21.2\n",
            "398 Episode: Finished after 18 steps：10試行の平均step数 = 21.9\n",
            "399 Episode: Finished after 40 steps：10試行の平均step数 = 24.6\n",
            "400 Episode: Finished after 24 steps：10試行の平均step数 = 25.6\n",
            "401 Episode: Finished after 15 steps：10試行の平均step数 = 24.8\n",
            "402 Episode: Finished after 45 steps：10試行の平均step数 = 24.7\n",
            "403 Episode: Finished after 12 steps：10試行の平均step数 = 24.2\n",
            "404 Episode: Finished after 17 steps：10試行の平均step数 = 24.8\n",
            "405 Episode: Finished after 23 steps：10試行の平均step数 = 25.9\n",
            "406 Episode: Finished after 23 steps：10試行の平均step数 = 23.9\n",
            "407 Episode: Finished after 15 steps：10試行の平均step数 = 23.2\n",
            "408 Episode: Finished after 22 steps：10試行の平均step数 = 23.6\n",
            "409 Episode: Finished after 14 steps：10試行の平均step数 = 21.0\n",
            "410 Episode: Finished after 15 steps：10試行の平均step数 = 20.1\n",
            "411 Episode: Finished after 12 steps：10試行の平均step数 = 19.8\n",
            "412 Episode: Finished after 45 steps：10試行の平均step数 = 19.8\n",
            "413 Episode: Finished after 17 steps：10試行の平均step数 = 20.3\n",
            "414 Episode: Finished after 17 steps：10試行の平均step数 = 20.3\n",
            "415 Episode: Finished after 21 steps：10試行の平均step数 = 20.1\n",
            "416 Episode: Finished after 21 steps：10試行の平均step数 = 19.9\n",
            "417 Episode: Finished after 20 steps：10試行の平均step数 = 20.4\n",
            "418 Episode: Finished after 19 steps：10試行の平均step数 = 20.1\n",
            "419 Episode: Finished after 16 steps：10試行の平均step数 = 20.3\n",
            "420 Episode: Finished after 20 steps：10試行の平均step数 = 20.8\n",
            "421 Episode: Finished after 10 steps：10試行の平均step数 = 20.6\n",
            "422 Episode: Finished after 19 steps：10試行の平均step数 = 18.0\n",
            "423 Episode: Finished after 13 steps：10試行の平均step数 = 17.6\n",
            "424 Episode: Finished after 11 steps：10試行の平均step数 = 17.0\n",
            "425 Episode: Finished after 24 steps：10試行の平均step数 = 17.3\n",
            "426 Episode: Finished after 19 steps：10試行の平均step数 = 17.1\n",
            "427 Episode: Finished after 36 steps：10試行の平均step数 = 18.7\n",
            "428 Episode: Finished after 10 steps：10試行の平均step数 = 17.8\n",
            "429 Episode: Finished after 13 steps：10試行の平均step数 = 17.5\n",
            "430 Episode: Finished after 17 steps：10試行の平均step数 = 17.2\n",
            "431 Episode: Finished after 20 steps：10試行の平均step数 = 18.2\n",
            "432 Episode: Finished after 17 steps：10試行の平均step数 = 18.0\n",
            "433 Episode: Finished after 10 steps：10試行の平均step数 = 17.7\n",
            "434 Episode: Finished after 16 steps：10試行の平均step数 = 18.2\n",
            "435 Episode: Finished after 9 steps：10試行の平均step数 = 16.7\n",
            "436 Episode: Finished after 24 steps：10試行の平均step数 = 17.2\n",
            "437 Episode: Finished after 11 steps：10試行の平均step数 = 14.7\n",
            "438 Episode: Finished after 16 steps：10試行の平均step数 = 15.3\n",
            "439 Episode: Finished after 12 steps：10試行の平均step数 = 15.2\n",
            "440 Episode: Finished after 28 steps：10試行の平均step数 = 16.3\n",
            "441 Episode: Finished after 36 steps：10試行の平均step数 = 17.9\n",
            "442 Episode: Finished after 15 steps：10試行の平均step数 = 17.7\n",
            "443 Episode: Finished after 15 steps：10試行の平均step数 = 18.2\n",
            "444 Episode: Finished after 40 steps：10試行の平均step数 = 20.6\n",
            "445 Episode: Finished after 11 steps：10試行の平均step数 = 20.8\n",
            "446 Episode: Finished after 23 steps：10試行の平均step数 = 20.7\n",
            "447 Episode: Finished after 12 steps：10試行の平均step数 = 20.8\n",
            "448 Episode: Finished after 27 steps：10試行の平均step数 = 21.9\n",
            "449 Episode: Finished after 38 steps：10試行の平均step数 = 24.5\n",
            "450 Episode: Finished after 20 steps：10試行の平均step数 = 23.7\n",
            "451 Episode: Finished after 35 steps：10試行の平均step数 = 23.6\n",
            "452 Episode: Finished after 34 steps：10試行の平均step数 = 25.5\n",
            "453 Episode: Finished after 17 steps：10試行の平均step数 = 25.7\n",
            "454 Episode: Finished after 18 steps：10試行の平均step数 = 23.5\n",
            "455 Episode: Finished after 17 steps：10試行の平均step数 = 24.1\n",
            "456 Episode: Finished after 13 steps：10試行の平均step数 = 23.1\n",
            "457 Episode: Finished after 63 steps：10試行の平均step数 = 28.2\n",
            "458 Episode: Finished after 13 steps：10試行の平均step数 = 26.8\n",
            "459 Episode: Finished after 31 steps：10試行の平均step数 = 26.1\n",
            "460 Episode: Finished after 14 steps：10試行の平均step数 = 25.5\n",
            "461 Episode: Finished after 38 steps：10試行の平均step数 = 25.8\n",
            "462 Episode: Finished after 17 steps：10試行の平均step数 = 24.1\n",
            "463 Episode: Finished after 14 steps：10試行の平均step数 = 23.8\n",
            "464 Episode: Finished after 22 steps：10試行の平均step数 = 24.2\n",
            "465 Episode: Finished after 23 steps：10試行の平均step数 = 24.8\n",
            "466 Episode: Finished after 16 steps：10試行の平均step数 = 25.1\n",
            "467 Episode: Finished after 31 steps：10試行の平均step数 = 21.9\n",
            "468 Episode: Finished after 27 steps：10試行の平均step数 = 23.3\n",
            "469 Episode: Finished after 14 steps：10試行の平均step数 = 21.6\n",
            "470 Episode: Finished after 10 steps：10試行の平均step数 = 21.2\n",
            "471 Episode: Finished after 30 steps：10試行の平均step数 = 20.4\n",
            "472 Episode: Finished after 19 steps：10試行の平均step数 = 20.6\n",
            "473 Episode: Finished after 49 steps：10試行の平均step数 = 24.1\n",
            "474 Episode: Finished after 17 steps：10試行の平均step数 = 23.6\n",
            "475 Episode: Finished after 21 steps：10試行の平均step数 = 23.4\n",
            "476 Episode: Finished after 18 steps：10試行の平均step数 = 23.6\n",
            "477 Episode: Finished after 55 steps：10試行の平均step数 = 26.0\n",
            "478 Episode: Finished after 29 steps：10試行の平均step数 = 26.2\n",
            "479 Episode: Finished after 36 steps：10試行の平均step数 = 28.4\n",
            "480 Episode: Finished after 12 steps：10試行の平均step数 = 28.6\n",
            "481 Episode: Finished after 43 steps：10試行の平均step数 = 29.9\n",
            "482 Episode: Finished after 22 steps：10試行の平均step数 = 30.2\n",
            "483 Episode: Finished after 29 steps：10試行の平均step数 = 28.2\n",
            "484 Episode: Finished after 14 steps：10試行の平均step数 = 27.9\n",
            "485 Episode: Finished after 37 steps：10試行の平均step数 = 29.5\n",
            "486 Episode: Finished after 18 steps：10試行の平均step数 = 29.5\n",
            "487 Episode: Finished after 26 steps：10試行の平均step数 = 26.6\n",
            "488 Episode: Finished after 28 steps：10試行の平均step数 = 26.5\n",
            "489 Episode: Finished after 18 steps：10試行の平均step数 = 24.7\n",
            "490 Episode: Finished after 33 steps：10試行の平均step数 = 26.8\n",
            "491 Episode: Finished after 27 steps：10試行の平均step数 = 25.2\n",
            "492 Episode: Finished after 37 steps：10試行の平均step数 = 26.7\n",
            "493 Episode: Finished after 27 steps：10試行の平均step数 = 26.5\n",
            "494 Episode: Finished after 44 steps：10試行の平均step数 = 29.5\n",
            "495 Episode: Finished after 13 steps：10試行の平均step数 = 27.1\n",
            "496 Episode: Finished after 23 steps：10試行の平均step数 = 27.6\n",
            "497 Episode: Finished after 15 steps：10試行の平均step数 = 26.5\n",
            "498 Episode: Finished after 12 steps：10試行の平均step数 = 24.9\n",
            "499 Episode: Finished after 38 steps：10試行の平均step数 = 26.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUszAbUoRFTk"
      },
      "source": [
        "# BBB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 919
        },
        "id": "AnZAifgARH-x",
        "outputId": "3147e6c9-012f-47d8-d611-4d27a6319cb6"
      },
      "source": [
        "import gym\n",
        "import collections\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "#Hyperparameters\n",
        "learning_rate = 0.0005\n",
        "gamma         = 0.98\n",
        "buffer_limit  = 50000\n",
        "batch_size    = 32\n",
        "\n",
        "class Memories():\n",
        "    def __init__(self):\n",
        "        self.memory = collections.deque(maxlen=buffer_limit)\n",
        "    \n",
        "    def put(self, transition):\n",
        "        self.memory.append(transition)\n",
        "    \n",
        "    def sample(self, n):\n",
        "        mini_batch = random.sample(self.memory, n)\n",
        "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
        "        \n",
        "        for transition in mini_batch:\n",
        "            s, a, r, s_prime, done_mask = transition\n",
        "            s_lst.append(s)\n",
        "            a_lst.append([a])\n",
        "            r_lst.append([r])\n",
        "            s_prime_lst.append(s_prime)\n",
        "            done_mask_lst.append([done_mask])\n",
        "\n",
        "        return torch.tensor(s_lst, dtype=torch.float), \\\n",
        "               torch.tensor(a_lst), \\\n",
        "               torch.tensor(r_lst), \\\n",
        "               torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
        "               torch.tensor(done_mask_lst)\n",
        "    \n",
        "    def size(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "class Qnet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Qnet, self).__init__()\n",
        "        self.fc1 = nn.Linear(4, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "      \n",
        "    def sample_action(self, obs, epsilon):\n",
        "        out = self.forward(obs)\n",
        "        coin = random.random()\n",
        "        if coin < epsilon:\n",
        "            return random.randint(0,1)\n",
        "        else : \n",
        "            return out.argmax().item()\n",
        "            \n",
        "def train(q, q_target, memory, optimizer):\n",
        "    for i in range(10):\n",
        "        s,a,r,s_prime,done_mask = memory.sample(batch_size)\n",
        "\n",
        "        q_out = q(s)\n",
        "        q_a = q_out.gather(1,a)\n",
        "        max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)\n",
        "        target = r + gamma * max_q_prime * done_mask\n",
        "        loss = F.smooth_l1_loss(q_a, target)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def main():\n",
        "    env = gym.make('CartPole-v1')\n",
        "    q = Qnet()\n",
        "    q_target = Qnet()\n",
        "    q_target.load_state_dict(q.state_dict())\n",
        "    memory = Memories()\n",
        "\n",
        "    print_interval = 20\n",
        "    score = 0.0  \n",
        "    optimizer = optim.Adam(q.parameters(), lr=learning_rate)\n",
        "\n",
        "    for n_epi in range(10000):\n",
        "        epsilon = max(0.01, 0.08 - 0.01*(n_epi/200)) #Linear annealing from 8% to 1%\n",
        "        s = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            a = q.sample_action( torch.from_numpy(s).float(), epsilon )      \n",
        "            s_prime, r, done, info = env.step(a)\n",
        "            done_mask = 0.0 if done else 1.0\n",
        "            memory.put( (s,a,r/100.0,s_prime, done_mask) )\n",
        "            s = s_prime\n",
        "\n",
        "            score += r\n",
        "            if done:\n",
        "                break\n",
        "            \n",
        "        if memory.size()>2000:\n",
        "            train(q, q_target, memory, optimizer)\n",
        "\n",
        "        if n_epi%print_interval==0 and n_epi!=0:\n",
        "            q_target.load_state_dict(q.state_dict())\n",
        "            print(\"n_episode :{}, score : {:.1f}, memory_size : {}, eps : {:.1f}%\".format(\n",
        "                                                            n_epi, score/print_interval, memory.size(), epsilon*100))\n",
        "            score = 0.0\n",
        "    env.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n_episode :20, score : 10.0, memory_size : 200, eps : 7.9%\n",
            "n_episode :40, score : 9.6, memory_size : 392, eps : 7.8%\n",
            "n_episode :60, score : 9.8, memory_size : 587, eps : 7.7%\n",
            "n_episode :80, score : 9.4, memory_size : 775, eps : 7.6%\n",
            "n_episode :100, score : 9.7, memory_size : 968, eps : 7.5%\n",
            "n_episode :120, score : 9.7, memory_size : 1162, eps : 7.4%\n",
            "n_episode :140, score : 10.1, memory_size : 1363, eps : 7.3%\n",
            "n_episode :160, score : 9.7, memory_size : 1557, eps : 7.2%\n",
            "n_episode :180, score : 9.8, memory_size : 1752, eps : 7.1%\n",
            "n_episode :200, score : 9.9, memory_size : 1951, eps : 7.0%\n",
            "n_episode :220, score : 10.2, memory_size : 2156, eps : 6.9%\n",
            "n_episode :240, score : 10.1, memory_size : 2358, eps : 6.8%\n",
            "n_episode :260, score : 14.4, memory_size : 2647, eps : 6.7%\n",
            "n_episode :280, score : 17.9, memory_size : 3005, eps : 6.6%\n",
            "n_episode :300, score : 48.4, memory_size : 3972, eps : 6.5%\n",
            "n_episode :320, score : 73.4, memory_size : 5440, eps : 6.4%\n",
            "n_episode :340, score : 108.6, memory_size : 7612, eps : 6.3%\n",
            "n_episode :360, score : 161.3, memory_size : 10839, eps : 6.2%\n",
            "n_episode :380, score : 230.8, memory_size : 15454, eps : 6.1%\n",
            "n_episode :400, score : 223.4, memory_size : 19923, eps : 6.0%\n",
            "n_episode :420, score : 274.8, memory_size : 25419, eps : 5.9%\n",
            "n_episode :440, score : 248.8, memory_size : 30395, eps : 5.8%\n",
            "n_episode :460, score : 161.1, memory_size : 33617, eps : 5.7%\n",
            "n_episode :480, score : 211.6, memory_size : 37848, eps : 5.6%\n",
            "n_episode :500, score : 213.1, memory_size : 42109, eps : 5.5%\n",
            "n_episode :520, score : 218.5, memory_size : 46479, eps : 5.4%\n",
            "n_episode :540, score : 235.0, memory_size : 50000, eps : 5.3%\n",
            "n_episode :560, score : 319.2, memory_size : 50000, eps : 5.2%\n",
            "n_episode :580, score : 178.2, memory_size : 50000, eps : 5.1%\n",
            "n_episode :600, score : 252.6, memory_size : 50000, eps : 5.0%\n",
            "n_episode :620, score : 191.6, memory_size : 50000, eps : 4.9%\n",
            "n_episode :640, score : 307.9, memory_size : 50000, eps : 4.8%\n",
            "n_episode :660, score : 290.4, memory_size : 50000, eps : 4.7%\n",
            "n_episode :680, score : 288.2, memory_size : 50000, eps : 4.6%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-29a73a3b7ab0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-39-29a73a3b7ab0>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_action\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m             \u001b[0ms_prime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mdone_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-29a73a3b7ab0>\u001b[0m in \u001b[0;36msample_action\u001b[0;34m(self, obs, epsilon)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mcoin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcoin\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}